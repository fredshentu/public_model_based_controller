{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from nn_policy import FeedForwardCritic\n",
    "from nn_policy import FeedForwardPolicy\n",
    "from rllab.envs.mujoco.half_cheetah_env import HalfCheetahEnv\n",
    "from rllab.exploration_strategies.ou_strategy import OUStrategy\n",
    "from sandbox.rocky.tf.algos.ddpg import DDPG as ShaneDDPG\n",
    "from sandbox.rocky.tf.envs.base import TfEnv\n",
    "from sandbox.rocky.tf.policies.deterministic_mlp_policy import \\\n",
    "    DeterministicMLPPolicy\n",
    "from sandbox.rocky.tf.q_functions.continuous_mlp_q_function import \\\n",
    "    ContinuousMLPQFunction\n",
    "\n",
    "from ddpg import DDPG as MyDDPG\n",
    "from testing_utils import are_np_arrays_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = TfEnv(HalfCheetahEnv())\n",
    "action_dim = env.action_dim\n",
    "obs_dim = env.observation_space.low.shape[0]\n",
    "\n",
    "batch_size = 2\n",
    "rewards = np.random.rand(batch_size)\n",
    "terminals = (np.random.rand(batch_size) > 0.5).astype(np.int)\n",
    "obs = np.random.rand(batch_size, obs_dim)\n",
    "actions = np.random.rand(batch_size, action_dim)\n",
    "next_obs = np.random.rand(batch_size, obs_dim)\n",
    "\n",
    "ddpg_params = dict(\n",
    "    batch_size=64,\n",
    "    n_epochs=0,\n",
    "    epoch_length=0,\n",
    "    eval_samples=0,\n",
    "    discount=0.99,\n",
    "    qf_learning_rate=1e-3,\n",
    "    policy_learning_rate=1e-4,\n",
    "    soft_target_tau=0.001,\n",
    "    replay_pool_size=1000000,\n",
    "    min_pool_size=1000,\n",
    "    scale_reward=0.1,\n",
    ")\n",
    "discount = ddpg_params['discount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15005835  0.81457649]\n",
      "[0 1]\n",
      "[[ 0.43511439  0.21486068  0.43619294  0.66923761  0.20440605  0.82207058\n",
      "   0.83291033  0.72373561  0.89668103  0.67410786  0.80799981  0.64763201\n",
      "   0.01083204  0.4382325   0.93362274  0.55795521  0.63737658  0.7260999\n",
      "   0.9175968   0.17842764]\n",
      " [ 0.41534872  0.5935848   0.63982088  0.23709139  0.9229585   0.80080515\n",
      "   0.99038569  0.92861875  0.28002253  0.97068026  0.24973167  0.93388785\n",
      "   0.99066874  0.4360376   0.57956691  0.67015587  0.19678966  0.18611555\n",
      "   0.22873158  0.39150123]]\n",
      "[[ 0.04384032  0.64044176  0.06986806  0.99731914  0.78400959  0.12711896]\n",
      " [ 0.90925847  0.96190726  0.1259375   0.01973137  0.47221903  0.60472708]]\n",
      "[[ 0.29052842  0.92648082  0.00907505  0.4897972   0.45359199  0.36603501\n",
      "   0.26034967  0.76724245  0.64317068  0.36499064  0.72187408  0.24276138\n",
      "   0.22878558  0.8248953   0.64472811  0.08181222  0.31025709  0.35683179\n",
      "   0.68326028  0.1779539 ]\n",
      " [ 0.93819824  0.93290809  0.15855846  0.27508406  0.55827918  0.51646106\n",
      "   0.30439037  0.35100247  0.65420072  0.16924955  0.09570054  0.53530208\n",
      "   0.23401812  0.57407776  0.31642575  0.36555799  0.50138211  0.34332719\n",
      "   0.62882041  0.24917595]]\n"
     ]
    }
   ],
   "source": [
    "print(rewards)\n",
    "print(terminals)\n",
    "print(obs)\n",
    "print(actions)\n",
    "print(next_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create my stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess_me = tf.Session()\n",
    "with sess_me.as_default():\n",
    "    es = OUStrategy(env_spec=env.spec)\n",
    "    ddpg_params['Q_weight_decay'] = 0.\n",
    "    qf_params = dict(\n",
    "        embedded_hidden_sizes=(100, ),\n",
    "        observation_hidden_sizes=(100, ),\n",
    "        hidden_nonlinearity=tf.nn.relu,\n",
    "    )\n",
    "    policy_params = dict(\n",
    "        observation_hidden_sizes=(100, 100),\n",
    "        hidden_nonlinearity=tf.nn.relu,\n",
    "        output_nonlinearity=tf.nn.tanh,\n",
    "    )\n",
    "    qf = FeedForwardCritic(\n",
    "        \"critic\",\n",
    "        env.observation_space.flat_dim,\n",
    "        env.action_space.flat_dim,\n",
    "        **qf_params\n",
    "    )\n",
    "    policy = FeedForwardPolicy(\n",
    "        \"actor\",\n",
    "        env.observation_space.flat_dim,\n",
    "        env.action_space.flat_dim,\n",
    "        **policy_params\n",
    "    )\n",
    "    my_algo = MyDDPG(\n",
    "        env,\n",
    "        es,\n",
    "        policy,\n",
    "        qf,\n",
    "        **ddpg_params\n",
    "    )\n",
    "    my_policy = my_algo.actor\n",
    "    my_qf = my_algo.critic\n",
    "    my_target_policy = my_algo.target_actor\n",
    "    my_target_qf = my_algo.target_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Shane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess_shane = tf.Session()\n",
    "with sess_shane.as_default():\n",
    "    es = OUStrategy(env_spec=env.spec)\n",
    "    policy = DeterministicMLPPolicy(\n",
    "        name=\"init_policy\",\n",
    "        env_spec=env.spec,\n",
    "        hidden_sizes=(100, 100),\n",
    "        hidden_nonlinearity=tf.nn.relu,\n",
    "        output_nonlinearity=tf.nn.tanh,\n",
    "    )\n",
    "    qf = ContinuousMLPQFunction(\n",
    "        name=\"qf\",\n",
    "        env_spec=env.spec,\n",
    "        hidden_sizes=(100, 100),\n",
    "    )\n",
    "    ddpg_params.pop('Q_weight_decay')\n",
    "    shane_algo = ShaneDDPG(\n",
    "        env,\n",
    "        policy,\n",
    "        qf,\n",
    "        es,\n",
    "        **ddpg_params\n",
    "    )\n",
    "    sess_shane.run(tf.initialize_all_variables())\n",
    "    shane_algo.init_opt()\n",
    "#     This initializes the optimizer parameters\n",
    "    sess_shane.run(tf.initialize_all_variables())\n",
    "    f_train_policy = shane_algo.opt_info['f_train_policy']\n",
    "    f_train_qf = shane_algo.opt_info['f_train_qf']\n",
    "    shane_target_qf = shane_algo.opt_info[\"target_qf\"]\n",
    "    shane_target_policy = shane_algo.opt_info[\"target_policy\"]\n",
    "    shane_policy = shane_algo.policy\n",
    "    shane_qf = shane_algo.qf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure stuff from Shane's algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sess_shane.as_default():\n",
    "    shane_policy_param_values = shane_policy.flat_to_params(\n",
    "        shane_policy.get_param_values()\n",
    "    )\n",
    "    shane_qf_param_values = shane_qf.flat_to_params(\n",
    "        shane_qf.get_param_values()\n",
    "    )\n",
    "    # TODO(vpong): why are these two necessary?\n",
    "    shane_target_policy.set_param_values(shane_policy.get_param_values())\n",
    "    shane_target_qf.set_param_values(shane_qf.get_param_values())\n",
    "\n",
    "    shane_actions, _ = shane_policy.get_actions(obs)\n",
    "    shane_qf_out = shane_qf.get_qval(obs, actions)\n",
    "    shane_next_actions, _ = shane_target_policy.get_actions(next_obs)\n",
    "    shane_next_target_qf_values = shane_target_qf.get_qval(next_obs, shane_next_actions)\n",
    "    shane_ys = rewards + (1. - terminals) * discount * shane_next_target_qf_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy things to my algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sess_me.as_default():\n",
    "    my_policy.set_param_values(shane_policy_param_values)\n",
    "    my_target_policy.set_param_values(shane_policy_param_values)\n",
    "    my_qf.set_param_values(shane_qf_param_values)\n",
    "    my_target_qf.set_param_values(shane_qf_param_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure stuff from my algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict = my_algo._update_feed_dict(rewards, terminals, obs,\n",
    "                                      actions, next_obs)\n",
    "\n",
    "\n",
    "my_actions = sess_me.run(\n",
    "        my_policy.output,\n",
    "        feed_dict=feed_dict\n",
    "    )\n",
    "my_qf_out = sess_me.run(\n",
    "        my_qf.output,\n",
    "        feed_dict=feed_dict\n",
    "    ).flatten()\n",
    "my_next_actions = sess_me.run(\n",
    "        my_target_policy.output,\n",
    "        feed_dict=feed_dict\n",
    "    )\n",
    "my_next_target_qf_values = sess_me.run(\n",
    "    my_algo.target_critic.output,\n",
    "    feed_dict=feed_dict).flatten()\n",
    "my_ys = sess_me.run(my_algo.ys, feed_dict=feed_dict).flatten()\n",
    "\n",
    "my_policy_loss = sess_me.run(\n",
    "    my_algo.actor_surrogate_loss,\n",
    "    feed_dict=feed_dict)\n",
    "my_qf_loss = sess_me.run(\n",
    "    my_algo.critic_loss,\n",
    "    feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that Shane and my params stayed the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "shane_policy = shane_algo.policy\n",
    "shane_qf = shane_algo.qf\n",
    "with sess_shane.as_default():\n",
    "    shane_policy_param_values_new = shane_policy.flat_to_params(\n",
    "        shane_policy.get_param_values()\n",
    "    )\n",
    "    shane_qf_param_values_new = shane_qf.flat_to_params(\n",
    "        shane_qf.get_param_values()\n",
    "    )\n",
    "    shane_target_policy_param_values_new = shane_target_policy.flat_to_params(\n",
    "        shane_target_policy.get_param_values()\n",
    "    )\n",
    "    shane_target_qf_param_values_new = shane_target_qf.flat_to_params(\n",
    "        shane_target_qf.get_param_values()\n",
    "    )\n",
    "my_policy_params_values_new = my_algo.actor.get_param_values()\n",
    "my_qf_params_values_new = my_algo.critic.get_param_values()\n",
    "my_target_policy_params_values_new = my_algo.target_actor.get_param_values()\n",
    "my_target_qf_params_values_new = my_algo.target_critic.get_param_values()\n",
    "print(all((a==b).all() for a, b in zip(shane_policy_param_values, shane_policy_param_values_new)))\n",
    "print(all((a==b).all() for a, b in zip(shane_policy_param_values, my_policy_params_values_new)))\n",
    "print(all((a==b).all() for a, b in zip(shane_policy_param_values, shane_target_policy_param_values_new)))\n",
    "print(all((a==b).all() for a, b in zip(shane_policy_param_values, my_target_policy_params_values_new)))\n",
    "print(all((a==b).all() for a, b in zip(shane_qf_param_values, shane_qf_param_values_new)))\n",
    "print(all((a==b).all() for a, b in zip(shane_qf_param_values, my_qf_params_values_new)))\n",
    "print(all((a==b).all() for a, b in zip(shane_qf_param_values, shane_target_qf_param_values_new)))\n",
    "print(all((a==b).all() for a, b in zip(shane_qf_param_values, my_target_qf_params_values_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check critic outputs are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07917806  0.00283957]\n",
      "[-0.07917806  0.00283957]\n",
      "[-0.07917813  0.00283952]\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3 = shane_qf_param_values\n",
    "output = np.matmul(obs, W1) + b1\n",
    "output = np.maximum(output, 0)\n",
    "output = np.hstack((output, actions))\n",
    "output = np.matmul(output, W2) + b2\n",
    "output = np.maximum(output, 0)\n",
    "output = np.matmul(output, W3) + b3\n",
    "expected_qf_out = output.flatten()\n",
    "\n",
    "print(my_qf_out)\n",
    "print(shane_qf_out)\n",
    "print(expected_qf_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check actor outputs are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20947778  0.04484395  0.08546824  0.01056851  0.00029767  0.0958475 ]\n",
      " [ 0.01458523 -0.0430692   0.10159081 -0.15388419 -0.06008253  0.18279688]]\n",
      "[[-0.20947778  0.04484395  0.08546824  0.01056851  0.00029767  0.0958475 ]\n",
      " [ 0.01458523 -0.0430692   0.10159081 -0.15388419 -0.06008253  0.18279688]]\n",
      "[[-0.21262505  0.04487398  0.0856773   0.01056885  0.00029774  0.09614267]\n",
      " [ 0.01458626 -0.04309584  0.10194247 -0.15511645 -0.06015505  0.18487474]]\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3 = shane_policy_param_values\n",
    "output = np.matmul(obs, W1) + b1\n",
    "output = np.maximum(output, 0)\n",
    "output = np.matmul(output, W2) + b2\n",
    "output = np.maximum(output, 0)\n",
    "output = np.matmul(output, W3) + b3\n",
    "expected_action = output\n",
    "\n",
    "print(my_actions)\n",
    "print(shane_actions)\n",
    "print(expected_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that next action outputs are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.086945   -0.01997953  0.02840678  0.09882895  0.02658396  0.11652762]\n",
      " [ 0.01991368 -0.0152898   0.01624201  0.11547601 -0.00939338  0.18017189]]\n",
      "[[-0.086945   -0.01997953  0.02840678  0.09882895  0.02658396  0.11652762]\n",
      " [ 0.01991368 -0.0152898   0.01624201  0.11547601 -0.00939338  0.18017189]]\n",
      "[[-0.08716509 -0.01998221  0.02841444  0.09915265  0.02659021  0.11705939]\n",
      " [ 0.0199163  -0.015291    0.01624345  0.1159935  -0.00939367  0.18216033]]\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3 = shane_policy_param_values\n",
    "output = np.matmul(next_obs, W1) + b1\n",
    "output = np.maximum(output, 0)\n",
    "output = np.matmul(output, W2) + b2\n",
    "output = np.maximum(output, 0)\n",
    "output = np.matmul(output, W3) + b3\n",
    "expected_next_action = output\n",
    "\n",
    "print(my_next_actions)\n",
    "print(shane_next_actions)\n",
    "print(expected_next_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check next critic outputs are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03675101  0.01799645]\n",
      "[-0.03675101  0.01799645]\n",
      "[-0.03672561  0.01806539]\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3 = shane_qf_param_values\n",
    "output = np.matmul(next_obs, W1) + b1\n",
    "output = np.maximum(output, 0)\n",
    "output = np.hstack((output, expected_next_action))\n",
    "output = np.matmul(output, W2) + b2\n",
    "output = np.maximum(output, 0)\n",
    "output = np.matmul(output, W3) + b3\n",
    "expected_target_qf_values = output.flatten()\n",
    "\n",
    "print(shane_next_target_qf_values)\n",
    "print(my_next_target_qf_values)\n",
    "print(expected_target_qf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11367485  0.81457649]\n",
      "[ 0.11367485  0.81457649]\n",
      "[ 0.11367485  0.81457651]\n",
      "[ 0.11367485  0.81457649]\n",
      "[ 0.11369999  0.81457649]\n"
     ]
    }
   ],
   "source": [
    "my_expected_ys = rewards + (1. - terminals) * discount * my_next_target_qf_values\n",
    "shane_expected_ys = rewards + (1. - terminals) * discount * shane_next_target_qf_values\n",
    "expected_ys = rewards + (1. - terminals) * discount * expected_target_qf_values\n",
    "print(shane_ys)\n",
    "print(shane_expected_ys)\n",
    "print(my_ys)\n",
    "print(my_expected_ys)\n",
    "print(expected_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check losses are the same\n",
    "Only do this once since it changes the params!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with sess_shane.as_default():\n",
    "    shane_policy_loss, _ = f_train_policy(obs)\n",
    "    shane_qf_loss, qval, _ = f_train_qf(shane_ys, obs, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0512864\n",
      "0.0512864\n"
     ]
    }
   ],
   "source": [
    "print(my_policy_loss)\n",
    "print(shane_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.348055\n",
      "0.348055\n"
     ]
    }
   ],
   "source": [
    "print(shane_qf_loss)\n",
    "print(my_qf_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
